{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63e60f7b1794e7c812e9834b1cf99a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short story about a robot learning to paint:\n",
      "\n",
      "\n",
      "Once upon a time in a bustling city filled with the hum of technology, there was a robot named Artie. Artie was no ordinary machine; he was designed with the ability to learn and adapt. His creators had programmed him with a passion for art, but Artie had yet to discover his own style.\n",
      "\n",
      "\n",
      "One day, while wandering through the city's art district, Artie\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32, trust_remote_code=True)\n",
    "\n",
    "# Prepare input\n",
    "prompt = \"Write a short story about a robot learning to paint:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "generated_text = tokenizer.decode(outputs[0], cache_position=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_model_size, W8A16LinearLayer, replace_linear_with_target_and_quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model before:\n",
      "\n",
      " Phi3ForCausalLM(\n",
      "  (model): Phi3Model(\n",
      "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x Phi3DecoderLayer(\n",
      "        (self_attn): Phi3Attention(\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "          (rotary_emb): Phi3RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Phi3MLP(\n",
      "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (activation_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Phi3RMSNorm()\n",
      "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_attention_layernorm): Phi3RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): Phi3RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Model before:\\n\\n\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14576.26171875,\n",
       " {'float32': 14576.26171875, 'float16': 0, 'int8': 0, 'other': 0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_size(model):\n",
    "    \"\"\"\n",
    "    Calculate the size of the model in megabytes.\n",
    "    Takes into account different parameter dtypes.\n",
    "    \"\"\"\n",
    "    total_bytes = 0\n",
    "    for param in model.parameters():\n",
    "        # Get bytes per element based on dtype\n",
    "        if param.dtype == torch.float32:\n",
    "            bytes_per_element = 4\n",
    "        elif param.dtype == torch.float16:\n",
    "            bytes_per_element = 2\n",
    "        elif param.dtype == torch.int8:\n",
    "            bytes_per_element = 1\n",
    "        else:\n",
    "            bytes_per_element = param.element_size()  # fallback for other dtypes\n",
    "            \n",
    "        # Calculate bytes for this parameter\n",
    "        param_bytes = param.numel() * bytes_per_element\n",
    "        total_bytes += param_bytes\n",
    "        \n",
    "    # Convert to megabytes\n",
    "    size_mb = total_bytes / (1024 ** 2)\n",
    "    return size_mb\n",
    "\n",
    "def get_detailed_model_size(model):\n",
    "    \"\"\"\n",
    "    Calculate and print detailed size information for each parameter type.\n",
    "    \"\"\"\n",
    "    size_dict = {\n",
    "        'float32': 0,\n",
    "        'float16': 0,\n",
    "        'int8': 0,\n",
    "        'other': 0\n",
    "    }\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        # Calculate size in MB\n",
    "        if param.dtype == torch.float32:\n",
    "            size_dict['float32'] += param.numel() * 4 / (1024 * 1024)\n",
    "        elif param.dtype == torch.float16:\n",
    "            size_dict['float16'] += param.numel() * 2 / (1024 * 1024)\n",
    "        elif param.dtype == torch.int8:\n",
    "            size_dict['int8'] += param.numel() * 1 / (1024 * 1024)\n",
    "        else:\n",
    "            size_dict['other'] += param.numel() * param.element_size() / (1024 * 1024)\n",
    "    \n",
    "    return size_dict\n",
    "\n",
    "get_model_size(model), get_detailed_model_size(model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_linear_with_target_and_quantize(model, \n",
    "                                        W8A16LinearLayer, [\"lm_head\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model after:\n",
      "\n",
      " Phi3ForCausalLM(\n",
      "  (model): Phi3Model(\n",
      "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x Phi3DecoderLayer(\n",
      "        (self_attn): Phi3Attention(\n",
      "          (o_proj): W8A16LinearLayer()\n",
      "          (qkv_proj): W8A16LinearLayer()\n",
      "          (rotary_emb): Phi3RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Phi3MLP(\n",
      "          (gate_up_proj): W8A16LinearLayer()\n",
      "          (down_proj): W8A16LinearLayer()\n",
      "          (activation_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Phi3RMSNorm()\n",
      "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_attention_layernorm): Phi3RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): Phi3RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Model after:\\n\\n\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(752.26171875, {'float32': 752.26171875, 'float16': 0, 'int8': 0, 'other': 0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_size(model), get_detailed_model_size(model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short story about a robot learning to paint:\n",
      "\n",
      "\n",
      "Once upon a time, in a bustling city filled with the hum of technology, there was a robot named Artie. Artie was no ordinary machine; he was designed with the latest AI, capable of learning and adapting. His creators had programmed him with a passion for art, but Artie had yet to discover his own style.\n",
      "\n",
      "\n",
      "One day, while wandering through the city'\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "generated_text = tokenizer.decode(outputs[0], cache_position=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  -->  Phi3Model(\n",
      "  (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
      "  (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x Phi3DecoderLayer(\n",
      "      (self_attn): Phi3Attention(\n",
      "        (o_proj): W8A16LinearLayer()\n",
      "        (qkv_proj): W8A16LinearLayer()\n",
      "        (rotary_emb): Phi3RotaryEmbedding()\n",
      "      )\n",
      "      (mlp): Phi3MLP(\n",
      "        (gate_up_proj): W8A16LinearLayer()\n",
      "        (down_proj): W8A16LinearLayer()\n",
      "        (activation_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Phi3RMSNorm()\n",
      "      (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (post_attention_layernorm): Phi3RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): Phi3RMSNorm()\n",
      ")\n",
      "lm_head  -->  Linear(in_features=3072, out_features=32064, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for name, child in model.named_children():\n",
    "    print(name, ' --> ',child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c273d85bc23f459a861c3c0a0c6782bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Footprint of the model in MBs:  15284.318208\n"
     ]
    }
   ],
   "source": [
    "previous_memory_footprint = model.get_memory_footprint()\n",
    "print(\"Footprint of the model in MBs: \", previous_memory_footprint/1e+6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Model Analysis:\n",
      "--------------------------------------------------\n",
      "Layer Types:\n",
      "- Phi3ForCausalLM: 1\n",
      "- Phi3Model: 1\n",
      "- Embedding: 1\n",
      "- Dropout: 65\n",
      "- ModuleList: 1\n",
      "- Phi3DecoderLayer: 32\n",
      "- Phi3Attention: 32\n",
      "- Linear: 129\n",
      "- Phi3RotaryEmbedding: 32\n",
      "- Phi3MLP: 32\n",
      "- SiLU: 32\n",
      "- Phi3RMSNorm: 65\n",
      "\n",
      "Memory Usage by dtype (MB):\n",
      "- float32: 14576.26 MB\n",
      "- float16: 0.00 MB\n",
      "- int8: 0.00 MB\n",
      "- other: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "print_model_info(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'float32': 14576.26171875, 'float16': 0.0, 'int8': 0.0, 'other': 0.0}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_detailed_model_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short story about a robot learning to paint:\n",
      "\n",
      "\n",
      "Once upon a time in a bustling city filled with the hum of technology, there was a robot named Artie. Artie was no ordinary machine; he was designed with the ability to learn and adapt. His creators had programmed him with a passion for art, but Artie had yet to discover his own style.\n",
      "\n",
      "\n",
      "One day, while wandering through the city's art district, Artie\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    # torch_dtype=\"float32\",\n",
    "    tokenizer=tokenizer,\n",
    "    # trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Write a short story about a robot learning to paint:\"\n",
    "response = generator(\n",
    "    prompt,\n",
    "    max_length=100,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "# Print the generated text\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pipe(prompt, max_new_tokens=20, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model.generate(**inputs, max_length=100)\n",
    "# generated_text = tokenizer.decode(outputs[0], cache_position=True)\n",
    "\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter model.embed_tokens.weight has dtype torch.float32\n",
      "Parameter model.layers.0.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.0.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.0.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.0.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.0.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.0.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.1.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.1.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.1.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.1.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.1.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.1.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.2.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.2.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.2.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.2.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.2.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.2.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.3.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.3.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.3.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.3.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.3.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.3.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.4.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.4.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.4.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.4.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.4.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.4.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.5.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.5.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.5.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.5.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.5.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.5.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.6.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.6.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.6.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.6.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.6.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.6.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.7.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.7.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.7.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.7.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.7.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.7.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.8.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.8.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.8.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.8.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.8.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.8.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.9.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.9.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.9.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.9.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.9.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.9.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.10.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.10.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.10.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.10.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.10.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.10.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.11.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.11.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.11.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.11.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.11.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.11.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.12.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.12.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.12.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.12.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.12.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.12.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.13.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.13.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.13.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.13.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.13.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.13.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.14.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.14.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.14.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.14.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.14.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.14.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.15.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.15.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.15.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.15.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.15.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.15.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.16.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.16.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.16.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.16.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.16.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.16.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.17.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.17.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.17.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.17.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.17.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.17.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.18.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.18.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.18.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.18.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.18.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.18.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.19.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.19.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.19.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.19.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.19.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.19.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.20.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.20.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.20.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.20.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.20.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.20.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.21.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.21.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.21.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.21.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.21.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.21.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.22.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.22.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.22.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.22.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.22.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.22.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.23.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.23.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.23.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.23.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.23.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.23.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.24.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.24.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.24.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.24.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.24.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.24.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.25.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.25.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.25.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.25.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.25.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.25.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.26.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.26.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.26.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.26.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.26.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.26.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.27.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.27.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.27.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.27.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.27.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.27.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.28.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.28.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.28.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.28.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.28.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.28.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.29.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.29.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.29.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.29.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.29.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.29.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.30.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.30.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.30.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.30.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.30.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.30.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.31.self_attn.o_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.31.self_attn.qkv_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.31.mlp.gate_up_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.31.mlp.down_proj.weight has dtype torch.float32\n",
      "Parameter model.layers.31.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.31.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.norm.weight has dtype torch.float32\n",
      "Parameter lm_head.weight has dtype torch.float32\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter {name} has dtype {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_linear_with_target_and_quantize(model, \n",
    "                                        W8A16LinearLayer,\n",
    "                                        []\n",
    "                                        ) # quanitize full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short story about a robot learning to paint:\n",
      "\n",
      "\n",
      "Once upon a time, in a bustling city filled with the hum of technology, there was a robot named Artie. Artie was no ordinary robot; he was designed with the latest AI, capable of learning and adapting. His creators had programmed him with a passion for art, but Artie had yet to discover his own style.\n",
      "\n",
      "\n",
      "One day, while wandering through the city'\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "prompt = \"Write a short story about a robot learning to paint:\"\n",
    "response = generator(\n",
    "    prompt,\n",
    "    max_length=100,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "# Print the generated text\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter model.embed_tokens.weight has dtype torch.float32\n",
      "Parameter model.layers.0.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.0.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.1.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.1.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.2.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.2.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.3.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.3.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.4.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.4.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.5.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.5.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.6.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.6.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.7.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.7.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.8.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.8.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.9.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.9.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.10.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.10.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.11.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.11.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.12.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.12.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.13.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.13.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.14.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.14.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.15.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.15.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.16.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.16.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.17.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.17.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.18.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.18.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.19.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.19.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.20.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.20.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.21.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.21.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.22.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.22.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.23.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.23.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.24.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.24.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.25.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.25.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.26.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.26.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.27.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.27.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.28.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.28.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.29.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.29.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.30.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.30.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.31.input_layernorm.weight has dtype torch.float32\n",
      "Parameter model.layers.31.post_attention_layernorm.weight has dtype torch.float32\n",
      "Parameter model.norm.weight has dtype torch.float32\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter {name} has dtype {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer model.layers.0.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.0.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.0.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.0.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.1.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.1.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.1.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.1.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.2.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.2.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.2.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.2.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.3.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.3.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.3.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.3.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.4.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.4.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.4.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.4.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.5.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.5.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.5.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.5.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.6.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.6.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.6.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.6.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.7.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.7.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.7.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.7.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.8.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.8.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.8.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.8.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.9.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.9.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.9.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.9.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.10.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.10.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.10.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.10.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.11.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.11.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.11.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.11.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.12.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.12.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.12.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.12.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.13.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.13.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.13.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.13.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.14.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.14.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.14.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.14.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.15.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.15.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.15.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.15.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.16.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.16.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.16.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.16.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.17.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.17.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.17.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.17.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.18.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.18.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.18.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.18.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.19.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.19.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.19.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.19.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.20.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.20.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.20.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.20.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.21.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.21.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.21.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.21.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.22.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.22.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.22.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.22.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.23.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.23.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.23.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.23.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.24.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.24.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.24.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.24.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.25.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.25.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.25.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.25.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.26.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.26.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.26.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.26.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.27.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.27.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.27.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.27.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.28.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.28.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.28.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.28.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.29.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.29.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.29.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.29.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.30.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.30.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.30.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.30.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.31.self_attn.o_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.31.self_attn.qkv_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.31.mlp.gate_up_proj: Type = W8A16LinearLayer\n",
      "Layer model.layers.31.mlp.down_proj: Type = W8A16LinearLayer\n",
      "Layer lm_head: Type = W8A16LinearLayer\n",
      "\n",
      "Quantization Summary:\n",
      "- Quantized layers (W8A16LinearLayer): 129\n",
      "- Total linear layers: 129\n",
      "- Quantization ratio: 100.00%\n",
      "- Model size: 3930.45 MB\n",
      "\n",
      "Model quantization status: Quantized\n"
     ]
    }
   ],
   "source": [
    "def check_model_quantization(model):\n",
    "    \"\"\"\n",
    "    Improved function to verify model quantization status\n",
    "    \"\"\"\n",
    "    def count_quantized_layers():\n",
    "        quantized_count = 0\n",
    "        total_count = 0\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, (torch.nn.Linear, W8A16LinearLayer)):\n",
    "                total_count += 1\n",
    "                if isinstance(module, W8A16LinearLayer):\n",
    "                    quantized_count += 1\n",
    "                print(f\"Layer {name}: Type = {type(module).__name__}\")\n",
    "        \n",
    "        return quantized_count, total_count\n",
    "    \n",
    "    def check_memory_usage():\n",
    "        param_size = 0\n",
    "        for param in model.parameters():\n",
    "            param_size += param.nelement() * param.element_size()\n",
    "        buffer_size = 0\n",
    "        for buffer in model.buffers():\n",
    "            buffer_size += buffer.nelement() * buffer.element_size()\n",
    "            \n",
    "        size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "        return size_all_mb\n",
    "    \n",
    "    # Run checks\n",
    "    quantized_count, total_count = count_quantized_layers()\n",
    "    model_size = check_memory_usage()\n",
    "    \n",
    "    print(f\"\\nQuantization Summary:\")\n",
    "    print(f\"- Quantized layers (W8A16LinearLayer): {quantized_count}\")\n",
    "    print(f\"- Total linear layers: {total_count}\")\n",
    "    print(f\"- Quantization ratio: {(quantized_count/total_count)*100:.2f}%\")\n",
    "    print(f\"- Model size: {model_size:.2f} MB\")\n",
    "    \n",
    "    # Consider the model quantized if most layers are quantized\n",
    "    is_quantized = (quantized_count / total_count) > 0.5\n",
    "    return is_quantized\n",
    "\n",
    "# Usage\n",
    "is_quantized = check_model_quantization(model)\n",
    "print(f\"\\nModel quantization status: {'Quantized' if is_quantized else 'Not quantized'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'float32': 380.514892578125, 'float16': 0.0, 'int8': 3549.9375, 'other': 0.0}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_detailed_model_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Model Analysis:\n",
      "--------------------------------------------------\n",
      "Layer Types:\n",
      "- Phi3ForCausalLM: 1\n",
      "- Phi3Model: 1\n",
      "- Embedding: 1\n",
      "- Dropout: 65\n",
      "- ModuleList: 1\n",
      "- Phi3DecoderLayer: 32\n",
      "- Phi3Attention: 32\n",
      "- W8A16LinearLayer: 129\n",
      "- Phi3RotaryEmbedding: 32\n",
      "- Phi3MLP: 32\n",
      "- SiLU: 32\n",
      "- Phi3RMSNorm: 65\n",
      "\n",
      "Memory Usage by dtype (MB):\n",
      "- float32: 380.51 MB\n",
      "- float16: 0.00 MB\n",
      "- int8: 3549.94 MB\n",
      "- other: 0.00 MB\n",
      "\n",
      "W8A16LinearLayer Properties:\n",
      "- int8_weights dtype: torch.int8\n",
      "- scales dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "def get_detailed_model_size(model):\n",
    "    \"\"\"\n",
    "    Get detailed memory usage by dtype, including buffers\n",
    "    \"\"\"\n",
    "    def dtype_size_bytes(dtype):\n",
    "        if dtype == torch.int8:\n",
    "            return 1\n",
    "        elif dtype in [torch.float16, torch.bfloat16]:\n",
    "            return 2\n",
    "        elif dtype == torch.float32:\n",
    "            return 4\n",
    "        else:\n",
    "            return 8  # default for other types\n",
    "\n",
    "    size_dict = {\n",
    "        'float32': 0,\n",
    "        'float16': 0,\n",
    "        'int8': 0,\n",
    "        'other': 0\n",
    "    }\n",
    "    \n",
    "    # Check parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        dtype = param.dtype\n",
    "        size_bytes = param.nelement() * dtype_size_bytes(dtype)\n",
    "        \n",
    "        if dtype == torch.float32:\n",
    "            size_dict['float32'] += size_bytes\n",
    "        elif dtype == torch.float16:\n",
    "            size_dict['float16'] += size_bytes\n",
    "        elif dtype == torch.int8:\n",
    "            size_dict['int8'] += size_bytes\n",
    "        else:\n",
    "            size_dict['other'] += size_bytes\n",
    "            \n",
    "    # Check buffers (important for quantized models)\n",
    "    for name, buffer in model.named_buffers():\n",
    "        dtype = buffer.dtype\n",
    "        size_bytes = buffer.nelement() * dtype_size_bytes(dtype)\n",
    "        \n",
    "        if dtype == torch.float32:\n",
    "            size_dict['float32'] += size_bytes\n",
    "        elif dtype == torch.float16:\n",
    "            size_dict['float16'] += size_bytes\n",
    "        elif dtype == torch.int8:\n",
    "            size_dict['int8'] += size_bytes\n",
    "        else:\n",
    "            size_dict['other'] += size_bytes\n",
    "    \n",
    "    # Convert to MB\n",
    "    for key in size_dict:\n",
    "        size_dict[key] = size_dict[key] / (1024 * 1024)\n",
    "        \n",
    "    return size_dict\n",
    "\n",
    "def print_model_info(model):\n",
    "    \"\"\"\n",
    "    Print detailed model information including quantization\n",
    "    \"\"\"\n",
    "    print(\"\\nDetailed Model Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Count layer types\n",
    "    layer_types = {}\n",
    "    for name, module in model.named_modules():\n",
    "        module_type = type(module).__name__\n",
    "        if module_type not in layer_types:\n",
    "            layer_types[module_type] = 0\n",
    "        layer_types[module_type] += 1\n",
    "    \n",
    "    print(\"Layer Types:\")\n",
    "    for layer_type, count in layer_types.items():\n",
    "        print(f\"- {layer_type}: {count}\")\n",
    "    \n",
    "    # Get memory usage by dtype\n",
    "    memory_usage = get_detailed_model_size(model)\n",
    "    print(\"\\nMemory Usage by dtype (MB):\")\n",
    "    for dtype, size in memory_usage.items():\n",
    "        print(f\"- {dtype}: {size:.2f} MB\")\n",
    "    \n",
    "    # Check specific W8A16LinearLayer properties\n",
    "    w8a16_layers = [m for m in model.modules() if isinstance(m, W8A16LinearLayer)]\n",
    "    if w8a16_layers:\n",
    "        sample_layer = w8a16_layers[0]\n",
    "        print(\"\\nW8A16LinearLayer Properties:\")\n",
    "        print(f\"- int8_weights dtype: {sample_layer.int8_weights.dtype}\")\n",
    "        print(f\"- scales dtype: {sample_layer.scales.dtype}\")\n",
    "        if sample_layer.bias is not None:\n",
    "            print(f\"- bias dtype: {sample_layer.bias.dtype}\")\n",
    "\n",
    "# Usage\n",
    "print_model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Footprint of the model in MBs:  4121.378048\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "new_memory_footprint = model.get_memory_footprint()\n",
    "print(\"Footprint of the model in MBs: \", new_memory_footprint/1e+6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import psutil\n",
    "import numpy as np\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.nn.utils.parametrize import remove_parametrizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cuda_availability():\n",
    "    \"\"\"Check CUDA availability and initialize it\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available. Please check your GPU installation.\")\n",
    "    \n",
    "    # Initialize CUDA\n",
    "    torch.cuda.init()\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    # Print CUDA information\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    return device\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process()\n",
    "    cpu_memory = process.memory_info().rss / (1024 * 1024 * 1024)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / (1024 * 1024 * 1024)\n",
    "        return cpu_memory, gpu_memory\n",
    "    return cpu_memory, 0\n",
    "\n",
    "def measure_inference_latency(model, tokenizer, input_text, num_runs=10):\n",
    "    \"\"\"Measure average inference latency\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Warmup\n",
    "        # for _ in range(5):\n",
    "        #     with torch.no_grad():\n",
    "        #         _ = model.generate(**inputs, max_new_tokens=20)\n",
    "        \n",
    "        # Measure latency\n",
    "        latencies = []\n",
    "        for _ in tqdm(range(num_runs), desc=\"Measuring Latency\", leave=False):\n",
    "            torch.cuda.synchronize()  # Ensure CUDA operations are complete\n",
    "            start_time = time.time()\n",
    "            with torch.no_grad():\n",
    "                _ = model.generate(**inputs, max_new_tokens=20)\n",
    "            torch.cuda.synchronize()  # Ensure CUDA operations are complete\n",
    "            latencies.append(time.time() - start_time)\n",
    "        \n",
    "        return np.mean(latencies)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def compute_perplexity(model, tokenizer, dataset, max_samples=3000):\n",
    "    \"\"\"Compute perplexity on dataset\"\"\"\n",
    "    try:\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_length = 0\n",
    "        \n",
    "        for i, sample in tqdm(enumerate(dataset), 'Calculating Perplexity', leave=False):\n",
    "            if i >= max_samples:\n",
    "                break\n",
    "                \n",
    "            inputs = tokenizer(sample['text'], return_tensors='pt', truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "                loss = outputs.loss\n",
    "                \n",
    "            total_loss += loss.item() * inputs['input_ids'].size(1)\n",
    "            total_length += inputs['input_ids'].size(1)\n",
    "        \n",
    "        return torch.exp(torch.tensor(total_loss / total_length))\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing perplexity: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_model(model_name, quantization=None):\n",
    "    \"\"\"Load model with specified quantization\"\"\"\n",
    "    try:\n",
    "        if quantization == \"8bit\":\n",
    "            return AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                load_in_8bit=True,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "        elif quantization == \"4bit\":\n",
    "            return AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                load_in_4bit=True,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "            # model.to(\"cuda\")\n",
    "            return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     # Load model and tokenizer\n",
    "#     model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "#     print(f\"Loading {model_name}...\")\n",
    "    \n",
    "#     # Load original model\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#     original_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "#     original_model.to('cuda')\n",
    "    \n",
    "#     # Load dataset\n",
    "#     print(\"Loading Wikipedia dataset...\")\n",
    "#     # dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "#     dataset = load_dataset(\"ptb_text_only\", \"penn_treebank\", split=\"test\", trust_remote_code=True)\n",
    "#     # Ensure dataset size is at least 3000 points\n",
    "#     if len(dataset) > 3000:\n",
    "#         dataset = dataset.select(range(3000))\n",
    "#     else:\n",
    "#         print(f\"Warning: Dataset size ({len(dataset)}) is less than 3000 points\")\n",
    "    \n",
    "#     # Benchmark original model\n",
    "#     print(\"\\nBenchmarking original model (FP16)...\")\n",
    "#     original_memory = get_memory_usage()\n",
    "#     original_latency = measure_inference_latency(\n",
    "#         original_model, \n",
    "#         tokenizer, \n",
    "#         \"The quick brown fox jumps over the lazy dog\"\n",
    "#     )\n",
    "#     original_perplexity = compute_perplexity(original_model, tokenizer, dataset)\n",
    "    \n",
    "#     # Clear memory\n",
    "#     del original_model\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "    \n",
    "#     # Load and benchmark 8-bit model\n",
    "#     print(\"\\nLoading and benchmarking 8-bit model...\")\n",
    "#     model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_name,\n",
    "#         load_in_8bit=True,\n",
    "#         device_map=\"auto\"\n",
    "#     )\n",
    "    \n",
    "#     memory_8bit = get_memory_usage()\n",
    "#     latency_8bit = measure_inference_latency(\n",
    "#         model_8bit, \n",
    "#         tokenizer, \n",
    "#         \"The quick brown fox jumps over the lazy dog\"\n",
    "#     )\n",
    "#     perplexity_8bit = compute_perplexity(model_8bit, tokenizer, dataset)\n",
    "    \n",
    "#     # Clear memory\n",
    "#     del model_8bit\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "    \n",
    "#     # Load and benchmark 4-bit model\n",
    "#     print(\"\\nLoading and benchmarking 4-bit model...\")\n",
    "#     model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_name,\n",
    "#         load_in_4bit=True,\n",
    "#         device_map=\"auto\"\n",
    "#     )\n",
    "    \n",
    "#     memory_4bit = get_memory_usage()\n",
    "#     latency_4bit = measure_inference_latency(\n",
    "#         model_4bit, \n",
    "#         tokenizer, \n",
    "#         \"The quick brown fox jumps over the lazy dog\"\n",
    "#     )\n",
    "#     perplexity_4bit = compute_perplexity(model_4bit, tokenizer, dataset)\n",
    "    \n",
    "#     # Print results\n",
    "#     print(\"\\n=== Results ===\")\n",
    "#     print(\"\\nMemory Usage (GB):\")\n",
    "#     print(f\"Original (FP16): {original_memory:.2f}\")\n",
    "#     print(f\"8-bit: {memory_8bit:.2f}\")\n",
    "#     print(f\"4-bit: {memory_4bit:.2f}\")\n",
    "    \n",
    "#     print(\"\\nInference Latency (seconds):\")\n",
    "#     print(f\"Original (FP16): {original_latency:.4f}\")\n",
    "#     print(f\"8-bit: {latency_8bit:.4f}\")\n",
    "#     print(f\"4-bit: {latency_4bit:.4f}\")\n",
    "    \n",
    "#     print(\"\\nPerplexity:\")\n",
    "#     print(f\"Original (FP16): {original_perplexity:.2f}\")\n",
    "#     print(f\"8-bit: {perplexity_8bit:.2f}\")\n",
    "#     print(f\"4-bit: {perplexity_4bit:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Check CUDA availability and initialize\n",
    "        device = check_cuda_availability()\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Load dataset\n",
    "        print(\"Loading Wikipedia dataset...\")\n",
    "        \n",
    "        # Test input\n",
    "        test_input = \"The quick brown fox jumps over the lazy dog\"\n",
    "        # Load PTB dataset\n",
    "        dataset = load_dataset(\"ptb_text_only\", \"penn_treebank\", split=\"test\", trust_remote_code=True)\n",
    "        \n",
    "        # Ensure dataset size is at least 3000 points\n",
    "        if len(dataset) > 3000:\n",
    "            dataset = dataset.select(range(3000))\n",
    "        else:\n",
    "            print(f\"Warning: Dataset size ({len(dataset)}) is less than 3000 points\")\n",
    "        \n",
    "        # Dictionary to store results\n",
    "        results = {}\n",
    "        \n",
    "        # Test different quantization levels\n",
    "        for model_type in [\"original\"]: # \"8bit\", \"4bit\"\n",
    "            print(f\"\\nTesting {model_type} model...\")\n",
    "            \n",
    "            # Load appropriate model\n",
    "            model = load_model(model_name, model_type if model_type != \"original\" else None)\n",
    "            \n",
    "            # Measure metrics\n",
    "            cpu_mem, gpu_mem = get_memory_usage()\n",
    "            print('memory uasge computed.')\n",
    "            latency = measure_inference_latency(model, tokenizer, test_input)\n",
    "            print(f\"\\tCalculating perplexity for {model_type} model...\", end='\\r')\n",
    "            perplexity = compute_perplexity(model, tokenizer, dataset)\n",
    "            print(f\"\\tCalculated perplexity for {model_type} model               \")\n",
    "            \n",
    "            # Store results\n",
    "            results[model_type] = {\n",
    "                \"cpu_memory\": cpu_mem,\n",
    "                \"gpu_memory\": gpu_mem,\n",
    "                \"latency\": latency,\n",
    "                \"perplexity\": perplexity\n",
    "            }\n",
    "            \n",
    "            # Clean up\n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        # Print results\n",
    "        print(\"\\n=== Results ===\")\n",
    "        for model_type, metrics in results.items():\n",
    "            print(f\"\\n{model_type.upper()} MODEL:\")\n",
    "            print(f\"CPU Memory: {metrics['cpu_memory']:.2f} GB\")\n",
    "            print(f\"GPU Memory: {metrics['gpu_memory']:.2f} GB\")\n",
    "            print(f\"Inference Latency: {metrics['latency']:.4f} seconds\")\n",
    "            print(f\"Perplexity: {metrics['perplexity']:.2f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 2080 Ti\n",
      "CUDA Version: 12.1\n",
      "Loading microsoft/phi-3-mini-4k-instruct...\n",
      "Loading Wikipedia dataset...\n",
      "\n",
      "Testing original model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e4ee3d8f264cceb991c204815154ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 39\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Measure metrics\u001b[39;00m\n\u001b[1;32m     38\u001b[0m cpu_mem, gpu_mem \u001b[38;5;241m=\u001b[39m get_memory_usage()\n\u001b[0;32m---> 39\u001b[0m latency \u001b[38;5;241m=\u001b[39m \u001b[43mmeasure_inference_latency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mCalculating perplexity for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model...\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m compute_perplexity(model, tokenizer, dataset)\n",
      "Cell \u001b[0;32mIn[25], line 34\u001b[0m, in \u001b[0;36mmeasure_inference_latency\u001b[0;34m(model, tokenizer, input_text, num_runs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 34\u001b[0m         _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Measure latency\u001b[39;00m\n\u001b[1;32m     37\u001b[0m latencies \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/transformers/generation/utils.py:2047\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2040\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2041\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2042\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2043\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2044\u001b[0m     )\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2047\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2060\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2061\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2066\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2067\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/transformers/generation/utils.py:3007\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3004\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3007\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3010\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py:1266\u001b[0m, in \u001b[0;36mPhi3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1263\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1266\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1278\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py:1059\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1048\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1049\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1050\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         cache_position,\n\u001b[1;32m   1057\u001b[0m     )\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1059\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1069\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py:804\u001b[0m, in \u001b[0;36mPhi3DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    802\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    803\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 804\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_mlp_dropout(hidden_states)\n\u001b[1;32m    807\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py:344\u001b[0m, in \u001b[0;36mPhi3MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    341\u001b[0m gate, up_states \u001b[38;5;241m=\u001b[39m up_states\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    342\u001b[0m up_states \u001b[38;5;241m=\u001b[39m up_states \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(gate)\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mup_states\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anlp/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
