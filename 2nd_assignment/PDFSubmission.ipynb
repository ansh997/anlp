{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANLP24 Assignment2\n",
    "## Submitted by Himanshu Pal 2023701003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Q2.1: What is the purpose of self-attention, and how does it facilitate capturing\n",
    "dependencies in sequences?</span>\n",
    "\n",
    "<span style=\"color:green\">**Ans**:</span> Self-attention is a powerful mechanism in deep learning models that allows them to capture dependencies and relationships within input sequences, regardless of the distance between elements.\n",
    "The primary purpose of self-attention is to enable models to:\n",
    "- Focus on relevant parts of the input sequence\n",
    "- Capture long-range dependencies\n",
    "- Handle variable-length input sequences\n",
    "- Learn contextual representations of sequence elements\n",
    "\n",
    "**Attention Scores Computation**\n",
    "The model computes attention scores between each element in the input sequence and every other element. This is typically done using three components:\n",
    "\n",
    "**Query**: Represents the current focus or question about a specific element\n",
    "\n",
    "**Key**: Acts as a label or reference point for each element\n",
    "\n",
    "**Value**: Holds the actual information associated with each element\n",
    "\n",
    "Attention scores are calculated by comparing the query of one element with the keys of all other elements, usually through a dot product operation.\n",
    "\n",
    "**Weighted Information Aggregation**\n",
    "\n",
    "Once attention scores are computed, they are used to create a weighted sum of the value vectors. This process allows each element to gather information from all other elements in the sequence, weighted by their relevance.\n",
    "\n",
    "**Parallel Processing**\n",
    "Unlike traditional sequential models (e.g., RNNs), self-attention can process all elements of a sequence in parallel. This enables the model to consider the entire context simultaneously when computing representations for each element.\n",
    "\n",
    "**Multi-Head Attention**\n",
    "To capture different types of dependencies, transformer models often use multi-head attention. This involves running multiple self-attention operations in parallel, each with its own set of learned query, key, and value transformations. The results are then combined to produce the final output.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "Consider the sentence: \"The <span style=\"color:red\">cat</span>, which was chased by the dog, <span style=\"color:red\">ran</span> up the tree.\"\n",
    "Traditional models might struggle to connect \"cat\" with \"ran\" due to the intervening clause. However, self-attention can directly compute the relevance of \"cat\" to \"ran,\" effectively capturing this long-range dependency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Q2.2: Why do transformers use positional encodings in addition to word embeddings? Explain how positional encodings are incorporated into the transformer architecture. Briefly describe recent advances in various types of positional encodings used for transformers and how they differ from traditional\n",
    "sinusoidal positional encodings.</span>\n",
    "\n",
    "<span style=\"color:green\">Ans:</span> Transformers use positional encodings along with word embeddings because they help capture the order of words in a sequence, something the transformer architecture doesn’t inherently do since it processes inputs in parallel.\n",
    "\n",
    "**Purpose of Positional Encodings**:\n",
    "- **Capturing sequence order**: Since transformers don’t read sequences step-by-step like RNNs, positional encodings provide a way to include word order information.\n",
    "- **Distinguishing meaning**: The position of words can change a sentence’s meaning, and positional encodings help the model distinguish these variations.\n",
    "- **Position-aware attention**: These encodings let the model consider word positions when computing attention, making it aware of the relative relationships between words.\n",
    "\n",
    "Positional encodings are generated for each word in the sequence and added to the word embeddings, matching their dimensionality for element-wise combination. The combined inputs are then processed through the transformer's attention and feed-forward layers.\n",
    "\n",
    "**Recent Advances**:\n",
    "- **Relative Positional Encoding (RPE)**: Focuses on the distance between words instead of absolute positions, helping with long-range dependencies.\n",
    "- **Time Absolute Position Encoding (tAPE)**: Tailored for time series data, it adjusts based on sequence length and embedding size.\n",
    "- **Efficient Relative Position Encoding (eRPE)**: A faster and more efficient version of RPE, improving performance on time series tasks.\n",
    "- **Rotary Positional Encoding (RoPE)**: Uses a rotation operation to encode relative positions, preserving distances between embeddings.\n",
    "- **Axial Learned in Bins (ALiBi)**: Combines learned and fixed encodings, dividing the input into bins and applying different encodings to each.\n",
    "\n",
    "These new methods aim to improve on traditional sinusoidal encodings by better handling longer sequences and providing more efficient, flexible ways to capture positional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Q3.4: Write up a thorough analysis of the performance of your transformer model.\n",
    "Evaluate the quality of the model’s translations using the BLEU metric (you are also required to submit the BLEU scores for all sentences in the test set). Describe the hyperparameters you chose for the model and their significance. Provide explanations for the performance differences across different hyper-\n",
    "parameter configurations. Additionally, plot the loss curves to give insights into the training process.</span>\n",
    "\n",
    "<span style=\"color:green\">Ans:</span> The Performance of transformer model is reasonable to the size of training corpus. \n",
    "\n",
    "**Hyperparameter Tuning and Significance**: - Used wandb for Hyperparameter Tuning. Hyperparameter used - Batch size, Feedforward dimension, Dropout, Learning rate, Model dimension, Number of attention heads, Number of layers \n",
    "\n",
    "**Best Hyperparams**\n",
    "- batch_size:16\n",
    "- d_ff:4,096\n",
    "- dropout:0.5\n",
    "- learning_rate:0.00001\n",
    "- model_dim:512\n",
    "- num_heads:16\n",
    "- num_layers:8\n",
    "\n",
    "**Significance of Hyperparameters** - \n",
    "- Batch size: Smaller batch sizes help the model capture more variance from the data but can result in noisier gradients.\n",
    "- Feedforward dimension: Larger dimensions increase model capacity for capturing more complex relationships.\n",
    "- Dropout: A higher dropout rate helped prevent overfitting due to the small data size.\n",
    "- Learning rate: A lower learning rate prevented overshooting, especially with a deeper model.\n",
    "- Model dimension: 512 allowed a balanced level of semantic representation without overwhelming the data.\n",
    "- Attention heads: More heads allowed the model to attend to different positions in the input sequence simultaneously.\n",
    "- Number of layers: A deeper model improved learning but required more regularization to avoid overfitting.\n",
    "\n",
    "**Explanations for the Performance Differences Across Different Hyperparameter Configurations**\n",
    "\n",
    "- Larger feedforward dimensions and more attention heads provided better representational capacity but required regularization (dropout) to avoid overfitting.\n",
    "- Lower learning rates were crucial for stabilizing training, especially in deeper architectures.\n",
    "**Results** - \n",
    "- BLEU:10.124500734785611\n",
    "- epoch:10\n",
    "- ROUGE-L:0.03409134245225224\n",
    "- train_loss:9.965868473307292\n",
    "- valid_loss:9.92627111503056\n",
    "\n",
    "**Loss Plots and Scores**\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"validloss.png\" alt=\"Valid Loss\" width=\"400\"/></td>\n",
    "    <td><img src=\"train_loss.png\" alt=\"Train Loss\" width=\"400\"/></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"BLEU.png\" alt=\"BLEU\" width=\"400\"/></td>\n",
    "    <td><img src=\"ROUGE.png\" alt=\"ROUGE\" width=\"400\"/></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
