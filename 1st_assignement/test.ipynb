{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_language_model import TextDataset, dataloader, ft_embedding, NLM  # some issue with this\n",
    "\n",
    "from neural_language_model.preprocess_data import data_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# path_to_zip_file = '/scratch/hmnshpl/a\n",
    "# anlp_data/glove.6B.zip'\n",
    "# directory_to_extract_to = '/scratch/hmnshpl/anlp_data'\n",
    "# zip_ref = zipfile.ZipFile(path_to_zip_file, 'r')\n",
    "# zip_ref.extractall(directory_to_extract_to)\n",
    "# zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'loading embedding', end='\\r')\n",
    "embedding_model = ft_embedding.load_glove_embeddings()\n",
    "print(f'loaded embedding     ')\n",
    "\n",
    "file_path = '/scratch/hmnshpl/anlp_data/Auguste_Maquet.txt'  # Example path to dataset file\n",
    "embedding_dim = 300  # Make sure it matches the dimensionality of your pre-trained embeddings\n",
    "\n",
    "# Create the dataset instance\n",
    "dataset = TextDataset(file_path, embedding_model, embedding_dim=embedding_dim)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32  # Example batch size\n",
    "train_loader = create_dataloaders(dataset, batch_size=batch_size)\n",
    "\n",
    "# Example of iterating through the DataLoader\n",
    "for batch in train_loader:\n",
    "    contexts, targets = batch\n",
    "    print(\"Contexts Shape:\", contexts.shape)  # Should be (batch_size, context_size, embedding_dim)\n",
    "    print(\"Targets Shape:\", targets.shape)    # Should be (batch_size,)\n",
    "    break  # Remove this line to go through the whole dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a neural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import random_split\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scratch_location = '/scratch/hmnshpl/anlp_data'\n",
    "filename = 'Auguste_Maquet.txt'\n",
    "filepath = os.path.join(scratch_location, filename)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file_path = '/scratch/hmnshpl/anlp_data/glove.6B.300d.txt' \n",
    "embedding_dim = 300\n",
    "\n",
    "embedding_model = ft_embedding.load_glove_embeddings(glove_file_path, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "full_dataset = TextDataset(filepath, embedding_model)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "vocab_size = len(full_dataset.vocab)\n",
    "embedding_dim = full_dataset.embedding_dim\n",
    "hidden_dims = [300, 200]\n",
    "\n",
    "print(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print('Initializing model', end='\\r')\n",
    "model = NLM.NNLM(vocab_size, embedding_dim, hidden_dims[-1])\n",
    "print('Model Initialized    ')\n",
    "\n",
    "\n",
    "# Train model\n",
    "print('Training model', end='\\r')\n",
    "model.train_model(train_dataset, val_dataset, num_epochs=10, learning_rate=0.001)\n",
    "print('model training done')\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "print('Evaluating model', end=' ')\n",
    "test_perplexity = model.perplexity(test_dataset)\n",
    "print(f\"Test Perplexity: {test_perplexity}\")\n",
    "\n",
    "# Make a prediction\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "sample_context, _ = next(iter(test_loader))\n",
    "prediction = model.predict(sample_context)\n",
    "predicted_word_idx = prediction.argmax().item()\n",
    "predicted_word = full_dataset.idx_to_word[predicted_word_idx]\n",
    "print(f\"Predicted word: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
