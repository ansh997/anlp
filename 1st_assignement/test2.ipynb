{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import requests\n",
    "import re\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data preparation\n",
    "def get_auguste_maquet_corpus():\n",
    "    url = \"https://www.gutenberg.org/files/7849/7849-0.txt\"\n",
    "    response = requests.get(url)\n",
    "    text = response.text\n",
    "    # Remove header and footer\n",
    "    start = text.find(\"*** START OF THE PROJECT GUTENBERG EBOOK\")\n",
    "    end = text.find(\"*** END OF THE PROJECT GUTENBERG EBOOK\")\n",
    "    text = text[start:end]\n",
    "    # Clean text\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# 2. Tokenization and Vocabulary\n",
    "def tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, tokens):\n",
    "        self.itos = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"] + list(set(tokens))\n",
    "        self.stoi = {token: i for i, token in enumerate(self.itos)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    def encode(self, tokens):\n",
    "        return [self.stoi.get(token, self.stoi[\"<unk>\"]) for token in tokens]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return [self.itos[id] for id in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = get_auguste_maquet_corpus()\n",
    "tokens = tokenize(corpus)\n",
    "vocab = Vocabulary(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, vocab, seq_length):\n",
    "        self.text = text\n",
    "        self.vocab = vocab\n",
    "        self.seq_length = seq_length\n",
    "        self.tokens = tokenize(self.text)\n",
    "        self.data = self.vocab.encode(self.tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx+self.seq_length])\n",
    "        y = torch.tensor(self.data[idx+1:idx+self.seq_length+1])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model architecture\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, tgt, tgt_mask=None):\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.embedding.embedding_dim)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer_decoder(tgt, tgt, tgt_mask)\n",
    "        return self.fc_out(output)\n",
    "    \n",
    "    def calculate_loss(self, output, target):\n",
    "        return self.criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoder(\n",
      "  (embedding): Embedding(5108, 256)\n",
      "  (pos_encoder): PositionalEncoding()\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=256, out_features=5108, bias=True)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seq_length = 64\n",
    "batch_size = 32\n",
    "d_model = 256\n",
    "nhead = 8\n",
    "num_layers = 4\n",
    "dim_feedforward = 1024\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "\n",
    "dataset = TextDataset(corpus, vocab, seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = TransformerDecoder(len(vocab), d_model, nhead, num_layers, dim_feedforward).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:08<10:12, 68.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 6.1964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:16<09:04, 68.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 6.1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:23<07:55, 67.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 6.1682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [04:31<06:47, 67.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 6.1596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [05:40<05:40, 68.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 6.1568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [06:49<04:33, 68.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 6.1549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [07:58<03:25, 68.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 6.1555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [09:07<02:17, 68.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 6.1521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [10:15<01:08, 68.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 6.1494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [11:21<00:00, 68.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 6.1515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# 5. Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # print(x.shape)\n",
    "        output = model(x)\n",
    "        loss = criterion(output.view(-1, len(vocab)), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    loss = train()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# 6. Text generation\n",
    "def generate_text(model, start_sequence, max_length=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        current_sequence = start_sequence\n",
    "        for _ in range(max_length):\n",
    "            input_tensor = torch.tensor(vocab.encode(current_sequence)).unsqueeze(0).to(device)\n",
    "            output = model(input_tensor)\n",
    "            next_token_idx = output[0, -1, :].argmax().item()\n",
    "            next_token = vocab.itos[next_token_idx]\n",
    "            current_sequence.append(next_token)\n",
    "            if next_token == \"<eos>\":\n",
    "                break\n",
    "    return \" \".join(current_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the count of the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "start_sequence = [\"the\", \"count\", \"of\"]\n",
    "generated_text = generate_text(model, start_sequence)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the text generation function to calculate perplexity\n",
    "def generate_text_with_perplexity(model, start_sequence, max_length=10):\n",
    "    model.eval()\n",
    "    generated_sequence = start_sequence.copy()\n",
    "    total_log_likelihood = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            input_tensor = torch.tensor(vocab.encode(generated_sequence)).unsqueeze(0).to(device)\n",
    "            output = model(input_tensor)\n",
    "            next_token_logits = output[0, -1, :]\n",
    "            next_token_idx = next_token_logits.argmax().item()\n",
    "            next_token = vocab.itos[next_token_idx]\n",
    "            \n",
    "            # Calculate log likelihood of the chosen token\n",
    "            log_likelihood = -model.criterion(next_token_logits.unsqueeze(0), torch.tensor([next_token_idx]).to(device)).item()\n",
    "            total_log_likelihood += log_likelihood\n",
    "            total_tokens += 1\n",
    "            \n",
    "            generated_sequence.append(next_token)\n",
    "            if next_token == \"<eos>\" or len(generated_sequence) >= max_length:\n",
    "                break\n",
    "    \n",
    "    generated_text = \" \".join(generated_sequence)\n",
    "    perplexity = math.exp(-total_log_likelihood / total_tokens)\n",
    "    return generated_text, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "the count of the the the the the the the\n",
      "Text Perplexity: 16.5952\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "start_sequence = [\"the\", \"count\", \"of\"]\n",
    "generated_text, text_perplexity = generate_text_with_perplexity(model, start_sequence)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n",
    "print(f\"Text Perplexity: {text_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seq_length = 128  # Increased sequence length\n",
    "batch_size = 64   # Increased batch size\n",
    "d_model = 512     # Increased model dimension\n",
    "nhead = 8\n",
    "num_layers = 6    # Increased number of layers\n",
    "dim_feedforward = 2048\n",
    "lr = 0.0001       # Decreased learning rate\n",
    "epochs = 50       # Increased number of epochs\n",
    "\n",
    "dataset = TextDataset(corpus, vocab, seq_length)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "model = TransformerDecoder(len(vocab), d_model, nhead, num_layers, dim_feedforward).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 1/50 [03:36<2:56:50, 216.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train Loss: 6.1937, Train Perplexity: 489.6367\n",
      "Val Loss: 6.1833, Val Perplexity: 484.5922\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 2/50 [07:14<2:54:03, 217.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "Train Loss: 6.1852, Train Perplexity: 485.5127\n",
      "Val Loss: 6.1799, Val Perplexity: 482.9481\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 3/50 [10:52<2:50:36, 217.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "Train Loss: 6.1814, Train Perplexity: 483.6522\n",
      "Val Loss: 6.1774, Val Perplexity: 481.7247\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 4/50 [14:31<2:47:07, 217.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "Train Loss: 6.1783, Train Perplexity: 482.1849\n",
      "Val Loss: 6.1749, Val Perplexity: 480.5353\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 5/50 [18:09<2:43:29, 217.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "Train Loss: 6.1762, Train Perplexity: 481.1477\n",
      "Val Loss: 6.1736, Val Perplexity: 479.9243\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 6/50 [21:47<2:39:51, 217.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "Train Loss: 6.1743, Train Perplexity: 480.2290\n",
      "Val Loss: 6.1721, Val Perplexity: 479.1731\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 7/50 [25:25<2:36:11, 217.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "Train Loss: 6.1723, Train Perplexity: 479.2691\n",
      "Val Loss: 6.1708, Val Perplexity: 478.5728\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 8/50 [29:03<2:32:35, 217.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "Train Loss: 6.1709, Train Perplexity: 478.6131\n",
      "Val Loss: 6.1690, Val Perplexity: 477.6878\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 9/50 [32:41<2:28:57, 217.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "Train Loss: 6.1679, Train Perplexity: 477.1949\n",
      "Val Loss: 6.1667, Val Perplexity: 476.5965\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 10/50 [36:18<2:25:17, 217.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "Train Loss: 6.1667, Train Perplexity: 476.6041\n",
      "Val Loss: 6.1659, Val Perplexity: 476.2101\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 11/50 [39:56<2:21:36, 217.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "Train Loss: 6.1644, Train Perplexity: 475.5203\n",
      "Val Loss: 6.1630, Val Perplexity: 474.8523\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 12/50 [43:34<2:17:59, 217.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50\n",
      "Train Loss: 6.1635, Train Perplexity: 475.1067\n",
      "Val Loss: 6.1627, Val Perplexity: 474.7148\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 13/50 [47:12<2:14:22, 217.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      "Train Loss: 6.1631, Train Perplexity: 474.9070\n",
      "Val Loss: 6.1624, Val Perplexity: 474.5459\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 14/50 [50:50<2:10:49, 218.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n",
      "Train Loss: 6.1629, Train Perplexity: 474.8148\n",
      "Val Loss: 6.1622, Val Perplexity: 474.4508\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 15/50 [54:29<2:07:15, 218.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "Train Loss: 6.1628, Train Perplexity: 474.7598\n",
      "Val Loss: 6.1619, Val Perplexity: 474.3500\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 16/50 [58:07<2:03:38, 218.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      "Train Loss: 6.1626, Train Perplexity: 474.6569\n",
      "Val Loss: 6.1619, Val Perplexity: 474.3189\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 17/50 [1:01:45<2:00:00, 218.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n",
      "Train Loss: 6.1624, Train Perplexity: 474.5568\n",
      "Val Loss: 6.1617, Val Perplexity: 474.2389\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 18/50 [1:05:23<1:56:20, 218.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50\n",
      "Train Loss: 6.1620, Train Perplexity: 474.3818\n",
      "Val Loss: 6.1611, Val Perplexity: 473.9674\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 19/50 [1:09:01<1:52:40, 218.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n",
      "Train Loss: 6.1617, Train Perplexity: 474.2433\n",
      "Val Loss: 6.1610, Val Perplexity: 473.8885\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 20/50 [1:12:39<1:49:00, 218.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50\n",
      "Train Loss: 6.1616, Train Perplexity: 474.2050\n",
      "Val Loss: 6.1606, Val Perplexity: 473.7190\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 21/50 [1:16:17<1:45:20, 217.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "Train Loss: 6.1612, Train Perplexity: 474.0055\n",
      "Val Loss: 6.1606, Val Perplexity: 473.7122\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 22/50 [1:19:55<1:41:42, 217.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      "Train Loss: 6.1611, Train Perplexity: 473.9655\n",
      "Val Loss: 6.1606, Val Perplexity: 473.6917\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 23/50 [1:23:33<1:38:04, 217.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50\n",
      "Train Loss: 6.1612, Train Perplexity: 473.9776\n",
      "Val Loss: 6.1605, Val Perplexity: 473.6803\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 24/50 [1:27:11<1:34:25, 217.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n",
      "Train Loss: 6.1612, Train Perplexity: 473.9799\n",
      "Val Loss: 6.1605, Val Perplexity: 473.6793\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 25/50 [1:30:48<1:30:47, 217.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "Train Loss: 6.1612, Train Perplexity: 473.9826\n",
      "Val Loss: 6.1605, Val Perplexity: 473.6687\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 26/50 [1:34:26<1:27:08, 217.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "Train Loss: 6.1613, Train Perplexity: 474.0324\n",
      "Val Loss: 6.1605, Val Perplexity: 473.6613\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 27/50 [1:38:04<1:23:30, 217.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "Train Loss: 6.1611, Train Perplexity: 473.9391\n",
      "Val Loss: 6.1605, Val Perplexity: 473.6512\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 28/50 [1:41:42<1:19:52, 217.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "Train Loss: 6.1611, Train Perplexity: 473.9724\n",
      "Val Loss: 6.1605, Val Perplexity: 473.6478\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 29/50 [1:45:20<1:16:15, 217.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50\n",
      "Train Loss: 6.1610, Train Perplexity: 473.8867\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6382\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 30/50 [1:48:58<1:12:38, 217.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50\n",
      "Train Loss: 6.1611, Train Perplexity: 473.9620\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6372\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 31/50 [1:52:36<1:09:00, 217.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "Train Loss: 6.1609, Train Perplexity: 473.8575\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6335\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 32/50 [1:56:14<1:05:22, 217.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "Train Loss: 6.1609, Train Perplexity: 473.8449\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6331\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 33/50 [1:59:52<1:01:44, 217.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "Train Loss: 6.1610, Train Perplexity: 473.8961\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6328\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 34/50 [2:03:30<58:06, 217.92s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50\n",
      "Train Loss: 6.1610, Train Perplexity: 473.9014\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6312\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 35/50 [2:07:07<54:28, 217.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n",
      "Train Loss: 6.1610, Train Perplexity: 473.8817\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6308\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 36/50 [2:10:45<50:51, 217.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50\n",
      "Train Loss: 6.1610, Train Perplexity: 473.8861\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6299\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 37/50 [2:14:23<47:12, 217.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50\n",
      "Train Loss: 6.1608, Train Perplexity: 473.8289\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6292\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 38/50 [2:18:01<43:35, 217.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "Train Loss: 6.1609, Train Perplexity: 473.8492\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6288\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 39/50 [2:21:40<39:58, 218.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50\n",
      "Train Loss: 6.1610, Train Perplexity: 473.8931\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6287\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 40/50 [2:25:17<36:18, 217.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "Train Loss: 6.1609, Train Perplexity: 473.8773\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6264\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 41/50 [2:28:54<32:39, 217.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50\n",
      "Train Loss: 6.1609, Train Perplexity: 473.8694\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6265\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 42/50 [2:32:32<29:00, 217.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50\n",
      "Train Loss: 6.1609, Train Perplexity: 473.8561\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6267\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 43/50 [2:36:09<25:22, 217.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50\n",
      "Train Loss: 6.1609, Train Perplexity: 473.8317\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6265\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 44/50 [2:39:47<21:45, 217.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50\n",
      "Train Loss: 6.1609, Train Perplexity: 473.8528\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6265\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 45/50 [2:43:24<18:07, 217.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n",
      "Train Loss: 6.1610, Train Perplexity: 473.9023\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6267\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 46/50 [2:47:01<14:29, 217.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n",
      "Train Loss: 6.1611, Train Perplexity: 473.9275\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6268\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 47/50 [2:50:39<10:52, 217.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n",
      "Train Loss: 6.1609, Train Perplexity: 473.8597\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6267\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 48/50 [2:54:16<07:14, 217.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      "Train Loss: 6.1610, Train Perplexity: 473.8829\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6268\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 49/50 [2:57:54<03:37, 217.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "Train Loss: 6.1610, Train Perplexity: 473.9024\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6267\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [3:01:31<00:00, 217.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "Train Loss: 6.1609, Train Perplexity: 473.8779\n",
      "Val Loss: 6.1604, Val Perplexity: 473.6267\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient clipping\n",
    "clip_value = 1.0\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    for _, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = model.calculate_loss(output, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * y.numel()\n",
    "        total_tokens += y.numel()\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            loss = model.calculate_loss(output, y)\n",
    "            total_loss += loss.item() * y.numel()\n",
    "            total_tokens += y.numel()\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs), desc='Training'):\n",
    "    train_loss, train_perplexity = train_epoch(model, train_dataloader, optimizer, device)\n",
    "    val_loss, val_perplexity = validate(model, val_dataloader, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Perplexity: {train_perplexity:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Perplexity: {val_perplexity:.4f}\")\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "the count of had the he its the that for i had the it but as but a to and you i to the the a and and he had to he to so to and and the and the a the that the the the he to was but and k to and the was his the this for you of but the at and was at the had and i the to and him and and not the of to to and this it the a the the that it him in the and the the and to\n"
     ]
    }
   ],
   "source": [
    "# Improved text generation function\n",
    "def generate_text_with_sampling(model, start_sequence, max_length=100, temperature=0.7, top_k=50, top_p=0.9):\n",
    "    model.eval()\n",
    "    generated_sequence = start_sequence.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            input_tensor = torch.tensor(vocab.encode(generated_sequence)).unsqueeze(0).to(device)\n",
    "            output = model(input_tensor)\n",
    "            next_token_logits = output[0, -1, :] / temperature\n",
    "            \n",
    "            # Top-k sampling\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            \n",
    "            # Top-p (nucleus) sampling\n",
    "            sorted_logits, sorted_indices = torch.sort(top_k_logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            top_k_logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # Sample from the filtered distribution\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = top_k_indices[torch.multinomial(probs, 1).item()].item()\n",
    "            next_token = vocab.itos[next_token_idx]\n",
    "            \n",
    "            generated_sequence.append(next_token)\n",
    "            if next_token == \"<eos>\" or len(generated_sequence) >= max_length:\n",
    "                break\n",
    "    \n",
    "    return \" \".join(generated_sequence)\n",
    "\n",
    "# Example usage\n",
    "start_sequence = [\"the\", \"count\", \"of\"]\n",
    "generated_text = generate_text_with_sampling(model, start_sequence, temperature=0.7, top_k=50, top_p=0.9)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Token: and, Perplexity: 17.1910\n",
      "Generated Token: to, Perplexity: 8.6642\n",
      "Generated Token: said, Perplexity: 64.9131\n",
      "Generated Token: you, Perplexity: 35.3226\n",
      "Generated Token: the, Perplexity: 5.7980\n",
      "Generated Token: i, Perplexity: 50.8094\n",
      "Generated Token: the, Perplexity: 4.3551\n",
      "Generated Token: it, Perplexity: 27.9179\n",
      "Generated Token: and, Perplexity: 14.5621\n",
      "Generated Token: the, Perplexity: 5.3899\n",
      "Generated Token: with, Perplexity: 75.6919\n",
      "Generated Token: was, Perplexity: 43.3846\n",
      "Generated Token: he, Perplexity: 17.4259\n",
      "Generated Token: at, Perplexity: 122.5639\n",
      "Generated Token: it, Perplexity: 24.7624\n",
      "Generated Token: to, Perplexity: 7.1765\n",
      "Generated Token: on, Perplexity: 98.8945\n",
      "Generated Token: and, Perplexity: 17.3603\n",
      "Generated Token: to, Perplexity: 8.6642\n",
      "Generated Token: of, Perplexity: 18.0934\n",
      "Generated Token: of, Perplexity: 22.5940\n",
      "Generated Token: the, Perplexity: 3.9198\n",
      "Generated Token: at, Perplexity: 81.3594\n",
      "Generated Token: k, Perplexity: 32.5852\n",
      "Generated Token: it, Perplexity: 43.3017\n",
      "Generated Token: him, Perplexity: 85.8400\n",
      "Generated Token: had, Perplexity: 75.4379\n",
      "Generated Token: and, Perplexity: 15.3621\n",
      "Generated Token: it, Perplexity: 28.3879\n",
      "Generated Token: was, Perplexity: 20.4512\n",
      "Generated Token: and, Perplexity: 13.4640\n",
      "Generated Token: the, Perplexity: 5.3899\n",
      "Generated Token: but, Perplexity: 64.2193\n",
      "Generated Token: what, Perplexity: 131.2124\n",
      "Generated Token: he, Perplexity: 13.9960\n",
      "Generated Token: and, Perplexity: 21.1101\n",
      "Generated Token: his, Perplexity: 39.4325\n",
      "Generated Token: with, Perplexity: 47.2347\n",
      "Generated Token: a, Perplexity: 17.7035\n",
      "Generated Token: to, Perplexity: 8.0981\n",
      "Generated Token: of, Perplexity: 18.0934\n",
      "Generated Token: the, Perplexity: 3.9198\n",
      "Generated Token: the, Perplexity: 4.1602\n",
      "Generated Token: had, Perplexity: 58.7065\n",
      "Generated Token: the, Perplexity: 3.0409\n",
      "Generated Token: the, Perplexity: 4.1602\n",
      "Generated Token: was, Perplexity: 31.8789\n",
      "Generated Token: the, Perplexity: 5.7308\n",
      "Generated Token: the, Perplexity: 4.1602\n",
      "Generated Token: have, Perplexity: 119.9347\n",
      "Generated Token: it, Perplexity: 32.2629\n",
      "Generated Token: had, Perplexity: 54.9372\n",
      "Generated Token: the, Perplexity: 3.0409\n",
      "Generated Token: to, Perplexity: 9.5651\n",
      "Generated Token: he, Perplexity: 19.0270\n",
      "Generated Token: but, Perplexity: 56.9139\n",
      "Generated Token: his, Perplexity: 41.4356\n",
      "Generated Token: of, Perplexity: 21.1840\n",
      "Generated Token: not, Perplexity: 109.7712\n",
      "Generated Token: it, Perplexity: 26.4868\n",
      "Generated Token: the, Perplexity: 4.3903\n",
      "Generated Token: in, Perplexity: 30.7634\n",
      "Generated Token: k, Perplexity: 54.9965\n",
      "Generated Token: on, Perplexity: 81.1351\n",
      "Generated Token: in, Perplexity: 42.0291\n",
      "Generated Token: i, Perplexity: 48.8523\n",
      "Generated Token: in, Perplexity: 27.3342\n",
      "Generated Token: this, Perplexity: 88.2754\n",
      "Generated Token: with, Perplexity: 59.3690\n",
      "Generated Token: it, Perplexity: 27.9048\n",
      "Generated Token: the, Perplexity: 4.3903\n",
      "Generated Token: the, Perplexity: 4.1602\n",
      "Generated Token: to, Perplexity: 9.5651\n",
      "Generated Token: k, Perplexity: 36.1846\n",
      "Generated Token: but, Perplexity: 53.4890\n",
      "Generated Token: there, Perplexity: 83.3905\n",
      "Generated Token: you, Perplexity: 40.0303\n",
      "Generated Token: of, Perplexity: 14.8578\n",
      "Generated Token: the, Perplexity: 3.9198\n",
      "Generated Token: the, Perplexity: 4.1602\n",
      "Generated Token: the, Perplexity: 4.1602\n",
      "Generated Token: of, Perplexity: 17.7038\n",
      "Generated Token: the, Perplexity: 3.9198\n",
      "Generated Token: for, Perplexity: 77.1546\n",
      "Generated Token: but, Perplexity: 71.8965\n",
      "Generated Token: the, Perplexity: 4.8910\n",
      "Generated Token: i, Perplexity: 50.8094\n",
      "Generated Token: to, Perplexity: 8.2245\n",
      "Generated Token: a, Perplexity: 30.4752\n",
      "Generated Token: the, Perplexity: 5.2338\n",
      "Generated Token: his, Perplexity: 38.8741\n",
      "Generated Token: be, Perplexity: 110.5325\n",
      "Generated Token: it, Perplexity: 31.0264\n",
      "Generated Token: the, Perplexity: 4.3903\n",
      "Generated Token: the, Perplexity: 4.1602\n",
      "Generated Token: to, Perplexity: 9.5651\n",
      "Generated Token: of, Perplexity: 18.0934\n",
      "Generated Text:\n",
      "the count of and to said you the i the it and the with was he at it to on and to of of the at k it him had and it was and the but what he and his with a to of the the had the the was the the have it had the to he but his of not it the in k on in i in this with it the the to k but there you of the the the of the for but the i to a the his be it the the to of\n",
      "Perplexities: [17.19096822319588, 8.66422624251478, 64.91313265999692, 35.32260662721952, 5.798027047722692, 50.809437723220164, 4.355127706174269, 27.917942490602602, 14.562103453442521, 5.389899185252871, 75.69187831064171, 43.3846369174212, 17.42593226972585, 122.56394936184073, 24.76236993285986, 7.176496026927875, 98.89452113273424, 17.360300761760435, 8.664229039047015, 18.09341100609467, 22.593977648551984, 3.919794426636689, 81.35943108657854, 32.585237074237064, 43.30173686555063, 85.83998585298298, 75.43787490011213, 15.362073326699441, 28.38789836951889, 20.451196816007727, 13.463968878465112, 5.3898974536781035, 64.21929283658311, 131.21235012426317, 13.995956933183866, 21.11012777961799, 39.4325454304943, 47.23474892169781, 17.7034926078885, 8.098059739134527, 18.09340246922101, 3.919789389673029, 4.160190180507948, 58.706535181980435, 3.0409265526093727, 4.160190180507948, 31.878859234355268, 5.730771956845386, 4.160190180507948, 119.93470408993025, 32.262922995305665, 54.937158779446925, 3.040924899075926, 9.565110629197099, 19.027049218615378, 56.913910740286646, 41.43558038926549, 21.18404562775331, 109.77115634711113, 26.486836774424116, 4.390320850157403, 30.763427711711866, 54.996480403330736, 81.13511776790625, 42.029111022439935, 48.852301091706536, 27.33423335667909, 88.27543823590833, 59.36902774825816, 27.904844988477784, 4.390310223093161, 4.160203075403609, 9.565094950959708, 36.184578172056106, 53.48895207482922, 83.39047347743367, 40.03026866843269, 14.85777529664302, 3.9197953424496546, 4.160203075403609, 4.160203075403609, 17.703775161151334, 3.9197953424496546, 77.1546224898868, 71.8965255316801, 4.890968332648615, 50.80938963723784, 8.22450950222063, 30.47524037803833, 5.233765479195567, 38.874057665754634, 110.53252779598982, 31.026447477509663, 4.390310797527751, 4.160202043809014, 9.565102449240742, 18.093401249668297]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Improved text generation function with perplexity\n",
    "def generate_text_with_sampling_and_perplexity(model, start_sequence, vocab, max_length=100, temperature=0.7, top_k=50, top_p=0.9):\n",
    "    model.eval()\n",
    "    generated_sequence = start_sequence.copy()\n",
    "    perplexities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            input_tensor = torch.tensor(vocab.encode(generated_sequence)).unsqueeze(0).to(device)\n",
    "            output = model(input_tensor)\n",
    "            next_token_logits = output[0, -1, :] / temperature\n",
    "            \n",
    "            # Top-k sampling\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            \n",
    "            # Top-p (nucleus) sampling\n",
    "            sorted_logits, sorted_indices = torch.sort(top_k_logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            top_k_logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # Sample from the filtered distribution\n",
    "            probs = F.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = top_k_indices[torch.multinomial(probs, 1).item()].item()\n",
    "            next_token = vocab.itos[next_token_idx]\n",
    "            \n",
    "            # Calculate perplexity for the generated token\n",
    "            token_prob = probs[top_k_indices == next_token_idx].item()  # Probability of the sampled token\n",
    "            perplexity = math.exp(-math.log(token_prob)) if token_prob > 0 else float('inf')\n",
    "            perplexities.append(perplexity)\n",
    "            \n",
    "            generated_sequence.append(next_token)\n",
    "            print(f\"Generated Token: {next_token}, Perplexity: {perplexity:.4f}\")\n",
    "            \n",
    "            # Break if end-of-sequence token is generated or max length is reached\n",
    "            if next_token == \"<eos>\" or len(generated_sequence) >= max_length:\n",
    "                break\n",
    "    \n",
    "    return \" \".join(generated_sequence), perplexities\n",
    "\n",
    "# Example usage\n",
    "start_sequence = [\"the\", \"count\", \"of\"]\n",
    "generated_text, perplexities = generate_text_with_sampling_and_perplexity(model, start_sequence, vocab, temperature=0.7, top_k=50, top_p=0.9)\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n",
    "print(\"Perplexities:\", perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
